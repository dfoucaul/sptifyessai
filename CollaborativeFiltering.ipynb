{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1er essai, ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load dataset\n",
    "playlists_df = pd.read_csv(\"processed_first_50_files.csv\")\n",
    "\n",
    "# Encode playlists and tracks\n",
    "playlists_df['playlist_idx'] = playlists_df['playlist_id'].astype('category').cat.codes\n",
    "playlists_df['track_idx'] = playlists_df['track_uri'].astype('category').cat.codes\n",
    "\n",
    "# Create playlist-track interaction matrix\n",
    "interaction_matrix = csr_matrix(\n",
    "    (np.ones(len(playlists_df)), \n",
    "     (playlists_df['playlist_idx'], playlists_df['track_idx']))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split playlists into train and test sets\n",
    "playlist_ids = playlists_df['playlist_id'].unique()\n",
    "train_ids, test_ids = train_test_split(playlist_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create train and test datasets\n",
    "train_df = playlists_df[playlists_df['playlist_id'].isin(train_ids)]\n",
    "test_df = playlists_df[playlists_df['playlist_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Train Matrix Factorization Model (SVD)\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "playlist_embeddings = svd.fit_transform(interaction_matrix)\n",
    "\n",
    "# Predict missing tracks for playlists in the test set\n",
    "def recommend_tracks(playlist_idx, top_n=500):\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(playlist_embeddings[playlist_idx].reshape(1, -1), playlist_embeddings).flatten()\n",
    "    \n",
    "    # Rank tracks by similarity\n",
    "    similar_playlists = np.argsort(-similarities)[1:]  # Exclude itself\n",
    "    recommended_tracks = set()\n",
    "\n",
    "    for idx in similar_playlists:\n",
    "        if len(recommended_tracks) >= top_n:\n",
    "            break\n",
    "        recommended_tracks.update(train_df[train_df['playlist_idx'] == idx]['track_uri'])\n",
    "    \n",
    "    return list(recommended_tracks)[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Precision: 0.41030244417640205\n",
      "NDCG: 0.17023055036950918\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(test_df, top_n=500):\n",
    "    r_precision = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for playlist_id in test_df['playlist_id'].unique()[:10]:  # Limit to 5 for debugging\n",
    "        # Get ground truth and seed tracks\n",
    "        ground_truth = set(test_df[test_df['playlist_id'] == playlist_id]['track_uri'])\n",
    "        seed_tracks = list(ground_truth)[:max(1, len(ground_truth)//2)]  # Use 50% as seed\n",
    "        holdout_tracks = ground_truth - set(seed_tracks)\n",
    "\n",
    "        # Get recommendations\n",
    "        playlist_idx = test_df[test_df['playlist_id'] == playlist_id]['playlist_idx'].iloc[0]\n",
    "        recommended_tracks = recommend_tracks(playlist_idx, top_n)\n",
    "\n",
    "        # Calculate R-Precision\n",
    "        relevant_tracks = len(set(recommended_tracks) & holdout_tracks)\n",
    "        r_precision.append(relevant_tracks / len(holdout_tracks) if holdout_tracks else 0)\n",
    "\n",
    "        # Calculate NDCG\n",
    "        dcg = sum([1 / np.log2(i + 2) if track in holdout_tracks else 0 for i, track in enumerate(recommended_tracks)])\n",
    "        idcg = sum([1 / np.log2(i + 2) for i in range(len(holdout_tracks))])\n",
    "        ndcg_scores.append(dcg / idcg if idcg > 0 else 0)\n",
    "\n",
    "    # Report metrics\n",
    "    print(\"R-Precision:\", np.mean(r_precision))\n",
    "    print(\"NDCG:\", np.mean(ndcg_scores))\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2e essai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>track_position</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>album_name</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>track_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Throwbacks</td>\n",
       "      <td>0</td>\n",
       "      <td>Lose Control (feat. Ciara &amp; Fat Man Scoop)</td>\n",
       "      <td>Missy Elliott</td>\n",
       "      <td>The Cookbook</td>\n",
       "      <td>226863</td>\n",
       "      <td>spotify:track:0UaMYEvWZi0ZqiDOoHU3YI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Throwbacks</td>\n",
       "      <td>1</td>\n",
       "      <td>Toxic</td>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>In The Zone</td>\n",
       "      <td>198800</td>\n",
       "      <td>spotify:track:6I9VzXrHxO9rA9A5euc8Ak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Throwbacks</td>\n",
       "      <td>2</td>\n",
       "      <td>Crazy In Love</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Dangerously In Love (Alben für die Ewigkeit)</td>\n",
       "      <td>235933</td>\n",
       "      <td>spotify:track:0WqIKmW4BTrj3eJFmnCKMv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Throwbacks</td>\n",
       "      <td>3</td>\n",
       "      <td>Rock Your Body</td>\n",
       "      <td>Justin Timberlake</td>\n",
       "      <td>Justified</td>\n",
       "      <td>267266</td>\n",
       "      <td>spotify:track:1AWQoqb9bSvzTjaLralEkT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Throwbacks</td>\n",
       "      <td>4</td>\n",
       "      <td>It Wasn't Me</td>\n",
       "      <td>Shaggy</td>\n",
       "      <td>Hot Shot</td>\n",
       "      <td>227600</td>\n",
       "      <td>spotify:track:1lzr43nnXAijIGYnCT8M8H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playlist_id playlist_name  track_position  \\\n",
       "0            0    Throwbacks               0   \n",
       "1            0    Throwbacks               1   \n",
       "2            0    Throwbacks               2   \n",
       "3            0    Throwbacks               3   \n",
       "4            0    Throwbacks               4   \n",
       "\n",
       "                                   track_name        artist_name  \\\n",
       "0  Lose Control (feat. Ciara & Fat Man Scoop)      Missy Elliott   \n",
       "1                                       Toxic     Britney Spears   \n",
       "2                               Crazy In Love            Beyoncé   \n",
       "3                              Rock Your Body  Justin Timberlake   \n",
       "4                                It Wasn't Me             Shaggy   \n",
       "\n",
       "                                     album_name  duration_ms  \\\n",
       "0                                  The Cookbook       226863   \n",
       "1                                   In The Zone       198800   \n",
       "2  Dangerously In Love (Alben für die Ewigkeit)       235933   \n",
       "3                                     Justified       267266   \n",
       "4                                      Hot Shot       227600   \n",
       "\n",
       "                              track_uri  \n",
       "0  spotify:track:0UaMYEvWZi0ZqiDOoHU3YI  \n",
       "1  spotify:track:6I9VzXrHxO9rA9A5euc8Ak  \n",
       "2  spotify:track:0WqIKmW4BTrj3eJFmnCKMv  \n",
       "3  spotify:track:1AWQoqb9bSvzTjaLralEkT  \n",
       "4  spotify:track:1lzr43nnXAijIGYnCT8M8H  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = \"processed_first_50_files.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic info about the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_r_precision(recommended_tracks, ground_truth):\n",
    "    relevant_tracks = len(set(recommended_tracks) & set(ground_truth))\n",
    "    return relevant_tracks / len(ground_truth) if ground_truth else 0\n",
    "\n",
    "def compute_ndcg(recommended_tracks, ground_truth):\n",
    "    dcg = sum([1 / np.log2(i + 2) if track in ground_truth else 0 for i, track in enumerate(recommended_tracks)])\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(len(ground_truth))])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def compute_auc(recommended_scores, ground_truth_labels):\n",
    "    return roc_auc_score(ground_truth_labels, recommended_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Model - R-Precision: 0.038461538461538464, NDCG: 0.013957181036599698\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(playlist1, playlist2):\n",
    "    set1, set2 = set(playlist1), set(playlist2)\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# Generate recommendations\n",
    "def recommend_jaccard(playlist, all_playlists, top_n=500):\n",
    "    similarities = []\n",
    "    for pid, tracks in all_playlists.items():\n",
    "        similarity = jaccard_similarity(playlist, tracks)\n",
    "        similarities.append((pid, similarity))\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    recommended_playlists = similarities[:top_n]\n",
    "    recommended_tracks = {track for pid, _ in recommended_playlists for track in all_playlists[pid]}\n",
    "    return list(recommended_tracks)[:top_n]\n",
    "\n",
    "# Evaluate Jaccard Model\n",
    "# Create a dictionary: playlist_id -> list of track_uri\n",
    "all_playlists = data.groupby('playlist_id')['track_uri'].apply(list).to_dict()\n",
    "\n",
    "# Example: Evaluate a single playlist\n",
    "playlist_id = list(all_playlists.keys())[0]\n",
    "ground_truth = set(all_playlists[playlist_id])\n",
    "seed_tracks = list(ground_truth)[:max(1, len(ground_truth)//2)]\n",
    "holdout_tracks = ground_truth - set(seed_tracks)\n",
    "\n",
    "recommended_tracks = recommend_jaccard(seed_tracks, all_playlists)\n",
    "\n",
    "# Calculate metrics\n",
    "r_precision = compute_r_precision(recommended_tracks, holdout_tracks)\n",
    "ndcg = compute_ndcg(recommended_tracks, holdout_tracks)\n",
    "print(f\"Jaccard Model - R-Precision: {r_precision}, NDCG: {ndcg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item2Vec Model - R-Precision: 0.38461538461538464, NDCG: 0.17926785246016472\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Item2Vec\n",
    "sentences = data.groupby('playlist_id')['track_uri'].apply(list).tolist()\n",
    "\n",
    "# Train Word2Vec Model\n",
    "item2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, sg=1, workers=4)\n",
    "\n",
    "def recommend_item2vec(seed_tracks, all_tracks, top_n=500):\n",
    "    similarities = {}\n",
    "    for track in all_tracks:\n",
    "        if track not in seed_tracks:\n",
    "            similarity = sum(item2vec_model.wv.similarity(seed_track, track) for seed_track in seed_tracks if track in item2vec_model.wv)\n",
    "            similarities[track] = similarity\n",
    "    recommended_tracks = sorted(similarities.keys(), key=lambda x: similarities[x], reverse=True)\n",
    "    return recommended_tracks[:top_n]\n",
    "\n",
    "# Evaluate Item2Vec Model\n",
    "all_tracks = set(data['track_uri'])\n",
    "recommended_tracks = recommend_item2vec(seed_tracks, all_tracks)\n",
    "\n",
    "# Calculate metrics\n",
    "r_precision = compute_r_precision(recommended_tracks, holdout_tracks)\n",
    "ndcg = compute_ndcg(recommended_tracks, holdout_tracks)\n",
    "print(f\"Item2Vec Model - R-Precision: {r_precision}, NDCG: {ndcg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:143: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  num_cells = num_rows * num_columns\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1376000540 is out of bounds for axis 0 with size 1375963520",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare interaction matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m interaction_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplaylist_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrack_uri\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train SVD Model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m svd \u001b[38;5;241m=\u001b[39m TruncatedSVD(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:203\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m             to_unstack\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m--> 203\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43magged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_unstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dropna:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(table\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\series.py:4615\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[1;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[0;32m   4570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[0;32m   4572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4611\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[0;32m   4612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4613\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[1;32m-> 4615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:517\u001b[0m, in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[1;32m--> 517\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[0;32m    521\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:154\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[1;34m(self, index, level, constructor, sort)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[0;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    150\u001b[0m         PerformanceWarning,\n\u001b[0;32m    151\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    152\u001b[0m     )\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:207\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m selector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msorted_labels[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m stride \u001b[38;5;241m*\u001b[39m comp_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlift\n\u001b[0;32m    206\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(np\u001b[38;5;241m.\u001b[39mprod(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_shape), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1376000540 is out of bounds for axis 0 with size 1375963520"
     ]
    }
   ],
   "source": [
    "# Prepare interaction matrix\n",
    "interaction_matrix = pd.pivot_table(data, index='playlist_id', columns='track_uri', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Train SVD Model\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "playlist_embeddings = svd.fit_transform(interaction_matrix)\n",
    "\n",
    "def recommend_fism(playlist_idx, interaction_matrix, top_n=500):\n",
    "    similarities = cosine_similarity(playlist_embeddings[playlist_idx].reshape(1, -1), playlist_embeddings).flatten()\n",
    "    similar_playlists = np.argsort(-similarities)[1:]  # Exclude itself\n",
    "    recommended_tracks = set()\n",
    "    for idx in similar_playlists:\n",
    "        if len(recommended_tracks) >= top_n:\n",
    "            break\n",
    "        recommended_tracks.update(interaction_matrix.columns[interaction_matrix.iloc[idx] > 0])\n",
    "    return list(recommended_tracks)[:top_n]\n",
    "\n",
    "# Evaluate FISM Model\n",
    "playlist_idx = list(interaction_matrix.index).index(playlist_id)\n",
    "recommended_tracks = recommend_fism(playlist_idx, interaction_matrix)\n",
    "\n",
    "# Calculate metrics\n",
    "r_precision = compute_r_precision(recommended_tracks, holdout_tracks)\n",
    "ndcg = compute_ndcg(recommended_tracks, holdout_tracks)\n",
    "print(f\"FISM Model - R-Precision: {r_precision}, NDCG: {ndcg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3e essai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "         playlist_id playlist_name  track_position             track_name  \\\n",
      "3289146       142108      throwbax              17                     Oh   \n",
      "3126781        14679          viva              30               Abrázame   \n",
      "1203909       113981  random songs              67  Different Color Molly   \n",
      "2698683       134146         Venus              13        Kid Charlemagne   \n",
      "2796050       135659       Country              50        Californication   \n",
      "\n",
      "                   artist_name       album_name  duration_ms  \\\n",
      "3289146                  Ciara          Goodies       256346   \n",
      "3126781                 Camila      Todo Cambio       230720   \n",
      "1203909             Smokepurpp         Deadstar       185298   \n",
      "2698683             Steely Dan   The Royal Scam       278800   \n",
      "2796050  Red Hot Chili Peppers  Californication       329733   \n",
      "\n",
      "                                    track_uri  \n",
      "3289146  spotify:track:7i7UIbm5E0DD7aSOYvwp2v  \n",
      "3126781  spotify:track:6rCyXpDwlMf1GH2qlpjJWv  \n",
      "1203909  spotify:track:6gO7SsL92jgKRP8lsDGxYo  \n",
      "2698683  spotify:track:3sqNXrDvCy4nid6XbaA2Cg  \n",
      "2796050  spotify:track:48UPSzbZjgc449aqz8bxox  \n",
      "\n",
      "Testing Data:\n",
      "         playlist_id   playlist_name  track_position         track_name  \\\n",
      "2362910        13171            Hood              35               A-YO   \n",
      "2278347       128888  listen to this             158             Ghosts   \n",
      "2625534       133071             fun              34           Budapest   \n",
      "1776504       121433  September 2017              13          Polaroids   \n",
      "2996319       138720    Country Love              55  If You Told Me To   \n",
      "\n",
      "           artist_name              album_name  duration_ms  \\\n",
      "2362910     Method Man             Blackout! 2       223666   \n",
      "2278347  Mayday Parade  Monsters In The Closet       282906   \n",
      "2625534    George Ezra        Wanted On Voyage       200720   \n",
      "1776504     Jay Prince          Befor Our Time       277685   \n",
      "2996319   Hunter Hayes   Hunter Hayes (Encore)       206106   \n",
      "\n",
      "                                    track_uri  \n",
      "2362910  spotify:track:7s0hsZUXf9nIQrkTg3Da6p  \n",
      "2278347  spotify:track:1THOENDCBvX13qNkwctXlW  \n",
      "2625534  spotify:track:7GJClzimvMSghjcrKxuf1M  \n",
      "1776504  spotify:track:7rNOe6TJhf2INSLF1fdlSk  \n",
      "2996319  spotify:track:0X1WB4hNlLUOiZlrximBfn  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = \"processed_first_50_files.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display basic info about the datasets\n",
    "print(\"Training Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTesting Data:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_r_precision(recommended_tracks, ground_truth):\n",
    "    relevant_tracks = len(set(recommended_tracks) & set(ground_truth))\n",
    "    return relevant_tracks / len(ground_truth) if ground_truth else 0\n",
    "\n",
    "def compute_ndcg(recommended_tracks, ground_truth):\n",
    "    dcg = sum([1 / np.log2(i + 2) if track in ground_truth else 0 for i, track in enumerate(recommended_tracks)])\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(len(ground_truth))])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def compute_auc(recommended_scores, ground_truth_labels):\n",
    "    return roc_auc_score(ground_truth_labels, recommended_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Model - Playlist ID: 0\n",
      "R-Precision: 0.16666666666666666\n",
      "NDCG: 0.034345378034907025\n"
     ]
    }
   ],
   "source": [
    "# Compute Jaccard similarity\n",
    "def jaccard_similarity(playlist1, playlist2):\n",
    "    set1, set2 = set(playlist1), set(playlist2)\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# Generate recommendations using the training set\n",
    "def recommend_jaccard(playlist, train_playlists, top_n=500):\n",
    "    similarities = []\n",
    "    for pid, tracks in train_playlists.items():\n",
    "        similarity = jaccard_similarity(playlist, tracks)\n",
    "        similarities.append((pid, similarity))\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    recommended_playlists = similarities[:top_n]\n",
    "    recommended_tracks = {track for pid, _ in recommended_playlists for track in train_playlists[pid]}\n",
    "    return list(recommended_tracks)[:top_n]\n",
    "\n",
    "# Main function for evaluating a single playlist\n",
    "def evaluate_single_playlist(train_df, test_df):\n",
    "    # Prepare train and test playlists\n",
    "    train_playlists = train_df.groupby('playlist_id')['track_uri'].apply(list).to_dict()\n",
    "    test_playlists = test_df.groupby('playlist_id')['track_uri'].apply(list).to_dict()\n",
    "\n",
    "    # Select one playlist for evaluation\n",
    "    playlist_id = list(test_playlists.keys())[0]\n",
    "    tracks = test_playlists[playlist_id]\n",
    "    \n",
    "    # Split tracks into seed and holdout\n",
    "    seed_tracks = tracks[:max(1, len(tracks) // 2)]\n",
    "    holdout_tracks = set(tracks) - set(seed_tracks)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommended_tracks = recommend_jaccard(seed_tracks, train_playlists)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r_precision = len(set(recommended_tracks) & holdout_tracks) / len(holdout_tracks) if holdout_tracks else 0\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, track in enumerate(recommended_tracks) if track in holdout_tracks)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(len(holdout_tracks)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Jaccard Model - Playlist ID: {playlist_id}\")\n",
    "    print(f\"R-Precision: {r_precision}\")\n",
    "    print(f\"NDCG: {ndcg}\")\n",
    "\n",
    "# Call the function\n",
    "evaluate_single_playlist(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Item2Vec model...\n",
      "Item2Vec Model - Playlist ID: 0\n",
      "R-Precision: 0.5\n",
      "NDCG: 0.15014788133756604\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Item2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Train an Item2Vec model\n",
    "\n",
    "def train_item2vec(train_df, vector_size=50, window=5, min_count=1, epochs=10):\n",
    "    \"\"\"Train an Item2Vec model using tracks in playlists.\"\"\"\n",
    "    # Prepare sentences (playlists as lists of track URIs)\n",
    "    playlists = train_df.groupby('playlist_id')['track_uri'].apply(list).tolist()\n",
    "    \n",
    "    # Train Word2Vec (Item2Vec)\n",
    "    model = Word2Vec(sentences=playlists, vector_size=vector_size, window=window, min_count=min_count, sg=1, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# Generate recommendations using the trained Item2Vec model\n",
    "def recommend_item2vec(seed_tracks, model, train_playlists, top_n=500):\n",
    "    \"\"\"Generate recommendations for a playlist using the Item2Vec model.\"\"\"\n",
    "    recommended_tracks = set()\n",
    "\n",
    "    for track in seed_tracks:\n",
    "        if track in model.wv:\n",
    "            # Get most similar tracks for the seed track\n",
    "            similar_tracks = model.wv.most_similar(track, topn=top_n)\n",
    "            recommended_tracks.update([t[0] for t in similar_tracks])\n",
    "\n",
    "    # Filter out seed tracks\n",
    "    recommended_tracks -= set(seed_tracks)\n",
    "\n",
    "    return list(recommended_tracks)[:top_n]\n",
    "\n",
    "# Main function for evaluating a single playlist using Item2Vec\n",
    "def evaluate_single_playlist_item2vec(train_df, test_df):\n",
    "    # Train Item2Vec model\n",
    "    print(\"Training Item2Vec model...\")\n",
    "    item2vec_model = train_item2vec(train_df)\n",
    "\n",
    "    # Prepare train and test playlists\n",
    "    train_playlists = train_df.groupby('playlist_id')['track_uri'].apply(list).to_dict()\n",
    "    test_playlists = test_df.groupby('playlist_id')['track_uri'].apply(list).to_dict()\n",
    "\n",
    "    # Select one playlist for evaluation\n",
    "    playlist_id = list(test_playlists.keys())[0]\n",
    "    tracks = test_playlists[playlist_id]\n",
    "\n",
    "    # Split tracks into seed and holdout\n",
    "    seed_tracks = tracks[:max(1, len(tracks) // 2)]\n",
    "    holdout_tracks = set(tracks) - set(seed_tracks)\n",
    "\n",
    "    # Generate recommendations\n",
    "    recommended_tracks = recommend_item2vec(seed_tracks, item2vec_model, train_playlists)\n",
    "\n",
    "    # Calculate metrics\n",
    "    r_precision = len(set(recommended_tracks) & holdout_tracks) / len(holdout_tracks) if holdout_tracks else 0\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, track in enumerate(recommended_tracks) if track in holdout_tracks)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(len(holdout_tracks)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Item2Vec Model - Playlist ID: {playlist_id}\")\n",
    "    print(f\"R-Precision: {r_precision}\")\n",
    "    print(f\"NDCG: {ndcg}\")\n",
    "\n",
    "# Call the function\n",
    "evaluate_single_playlist_item2vec(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[43mevaluate_fism_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 65\u001b[0m, in \u001b[0;36mevaluate_fism_sparse\u001b[1;34m(train_df, test_df, alpha)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Create a sparse interaction matrix\u001b[39;00m\n\u001b[0;32m     64\u001b[0m rows, cols, data \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid, tracks \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaylist_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_uri\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m tracks:\n\u001b[0;32m     67\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend(playlist_to_idx[pid])\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\dask_expr\\_groupby.py:1615\u001b[0m, in \u001b[0;36mGroupBy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key):\n\u001b[1;32m-> 1615\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m            \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m            \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1623\u001b[0m     g \u001b[38;5;241m=\u001b[39m GroupBy(\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj,\n\u001b[0;32m   1625\u001b[0m         by\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys,\n\u001b[0;32m   1631\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\dask_expr\\_groupby.py:2214\u001b[0m, in \u001b[0;36mSeriesGroupBy.__init__\u001b[1;34m(self, obj, by, sort, observed, dropna, slice)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         obj\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mgroupby(by, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_as_dict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m\"\u001b[39m, observed))\n\u001b[1;32m-> 2214\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[0;32m   2216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\dask_expr\\_groupby.py:1558\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, by, group_keys, sort, observed, dropna, slice)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mslice\u001b[39m, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mslice\u001b[39m)\n\u001b[1;32m-> 1558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m     )\n\u001b[1;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\pandas\\core\\base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Column not found: 0'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate FISM embeddings using sparse matrices\n",
    "def generate_fism_embeddings_sparse(interaction_matrix, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Generate FISM embeddings for all playlists and tracks using a sparse matrix.\n",
    "\n",
    "    Parameters:\n",
    "        interaction_matrix (csr_matrix): A binary playlist-track interaction sparse matrix.\n",
    "        alpha (float): Neighborhood agreement value in the range of [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        csr_matrix: Track embeddings.\n",
    "    \"\"\"\n",
    "    row_sum = np.array(interaction_matrix.sum(axis=1)).flatten()\n",
    "    row_sum_alpha = np.power(row_sum, alpha)\n",
    "    row_sum_alpha[row_sum_alpha == 0] = 1  # Avoid division by zero\n",
    "\n",
    "    normalized_matrix = interaction_matrix.multiply(1 / row_sum_alpha[:, np.newaxis])\n",
    "    track_embeddings = normalized_matrix.T @ interaction_matrix\n",
    "\n",
    "    return track_embeddings\n",
    "\n",
    "def evaluate_fism_sparse(train_df, test_df, alpha=0.5):\n",
    "    # Debug: Print columns to verify\n",
    "    print(\"Train DataFrame Columns:\", train_df.columns)\n",
    "    print(\"Test DataFrame Columns:\", test_df.columns)\n",
    "\n",
    "    # Ensure the columns are correctly named\n",
    "    if 'playlist_id' not in train_df.columns or 'track_uri' not in train_df.columns:\n",
    "        raise KeyError(\"Missing required columns: 'playlist_id' or 'track_uri'\")\n",
    "\n",
    "    # Proceed with existing implementation...\n",
    "\n",
    "\n",
    "# Main evaluation function for FISM\n",
    "def evaluate_fism_sparse(train_df, test_df, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate FISM model using sparse matrices.\n",
    "\n",
    "    Parameters:\n",
    "        train_df (pd.DataFrame): Training data.\n",
    "        test_df (pd.DataFrame): Testing data.\n",
    "        alpha (float): Neighborhood agreement value for FISM.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a mapping of playlist and track IDs to sequential indices\n",
    "    playlist_ids = train_df['playlist_id'].unique()\n",
    "    playlist_to_idx = {pid: idx for idx, pid in enumerate(playlist_ids)}\n",
    "    idx_to_playlist = {idx: pid for pid, idx in playlist_to_idx.items()}\n",
    "\n",
    "    all_tracks = list(set(train_df['track_uri']).union(set(test_df['track_uri'])))\n",
    "    track_to_idx = {track: idx for idx, track in enumerate(all_tracks)}\n",
    "    idx_to_track = {idx: track for track, idx in track_to_idx.items()}\n",
    "\n",
    "    num_playlists = len(playlist_ids)\n",
    "    num_tracks = len(all_tracks)\n",
    "\n",
    "    # Create a sparse interaction matrix\n",
    "    rows, cols, data = [], [], []\n",
    "    for pid, tracks in train_df.groupby('playlist_id')['track_uri']:\n",
    "        for track in tracks:\n",
    "            rows.append(playlist_to_idx[pid])\n",
    "            cols.append(track_to_idx[track])\n",
    "            data.append(1)\n",
    "\n",
    "    interaction_matrix = csr_matrix((data, (rows, cols)), shape=(num_playlists, num_tracks))\n",
    "\n",
    "    # Generate FISM embeddings\n",
    "    track_embeddings = generate_fism_embeddings_sparse(interaction_matrix, alpha)\n",
    "\n",
    "    # Select a single test playlist\n",
    "    test_playlists = test_df.groupby('playlist_id')['track_uri'].apply(list).to_dict()\n",
    "    playlist_id = list(test_playlists.keys())[0]\n",
    "    tracks = test_playlists[playlist_id]\n",
    "\n",
    "    # Split tracks into seed and holdout\n",
    "    seed_tracks = tracks[:max(1, len(tracks) // 2)]\n",
    "    holdout_tracks = set(tracks) - set(seed_tracks)\n",
    "\n",
    "    # Find seed indices\n",
    "    seed_indices = [track_to_idx[track] for track in seed_tracks if track in track_to_idx]\n",
    "\n",
    "    # Recommend tracks\n",
    "    playlist_idx = playlist_to_idx[playlist_id]  # Map playlist_id to row index\n",
    "    scores = track_embeddings[:, playlist_idx].toarray().flatten()\n",
    "    recommended_indices = np.argsort(-scores)\n",
    "    recommended_tracks = [idx_to_track[idx] for idx in recommended_indices if idx in idx_to_track]\n",
    "\n",
    "    # Calculate metrics\n",
    "    r_precision = len(set(recommended_tracks) & holdout_tracks) / len(holdout_tracks) if holdout_tracks else 0\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, track in enumerate(recommended_tracks) if track in holdout_tracks)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(len(holdout_tracks)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"FISM Model - Playlist ID: {playlist_id}\")\n",
    "    print(f\"R-Precision: {r_precision}\")\n",
    "    print(f\"NDCG: {ndcg}\")\n",
    "\n",
    "# Example Usage\n",
    "evaluate_fism_sparse(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'dask_expr._collection.DataFrame'>\n",
      "<class 'dask_expr._collection.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"processed_first_50_files.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Confirm types\n",
    "print(type(train_df))  # Should be <class 'pandas.core.frame.DataFrame'>\n",
    "print(type(test_df))  # Should be <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "# Convert to Dask DataFrames\n",
    "train_df = dd.from_pandas(train_df, npartitions=4)\n",
    "test_df = dd.from_pandas(test_df, npartitions=2)\n",
    "\n",
    "# Confirm Dask DataFrame types\n",
    "print(type(train_df))  # Should be <class 'dask.dataframe.core.DataFrame'>\n",
    "print(type(test_df))  # Should be <class 'dask.dataframe.core.DataFrame'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'dask_expr._collection.DataFrame'>\n",
      "<class 'dask_expr._collection.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"processed_first_50_files.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Confirm types\n",
    "print(type(train_df))  # Should be <class 'pandas.core.frame.DataFrame'>\n",
    "print(type(test_df))  # Should be <class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "# Convert to Dask DataFrames\n",
    "train_df = dd.from_pandas(train_df, npartitions=4)\n",
    "test_df = dd.from_pandas(test_df, npartitions=2)\n",
    "\n",
    "# Confirm Dask DataFrame types\n",
    "print(type(train_df))  # Should be <class 'dask.dataframe.core.DataFrame'>\n",
    "print(type(test_df))  # Should be <class 'dask.dataframe.core.DataFrame'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input must be a pandas DataFrame or Series.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDCG: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Load Dask DataFrames\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m test_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(test_df, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\OneDrive - Université Libre de Bruxelles\\Documents\\Cours\\MA3\\Info\\Projet1\\SpotifyMillionChlg\\.venv\\lib\\site-packages\\dask_expr\\_collection.py:4874\u001b[0m, in \u001b[0;36mfrom_pandas\u001b[1;34m(data, npartitions, sort, chunksize)\u001b[0m\n\u001b[0;32m   4871\u001b[0m     npartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_parallel_type(data):\n\u001b[1;32m-> 4874\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a pandas DataFrame or Series.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_any_real_numeric_dtype(data\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m   4877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   4878\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex in passed data is non-numeric and contains nulls, which Dask does not entirely support.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4879\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider passing `data.loc[~data.isna()]` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4880\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Input must be a pandas DataFrame or Series."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Generate FISM embeddings using sparse matrices\n",
    "def generate_fism_embeddings_sparse(interaction_matrix, alpha=0.5):\n",
    "    row_sum = np.array(interaction_matrix.sum(axis=1)).flatten()\n",
    "    row_sum_alpha = np.power(row_sum, alpha)\n",
    "    row_sum_alpha[row_sum_alpha == 0] = 1  # Avoid division by zero\n",
    "\n",
    "    normalized_matrix = interaction_matrix.multiply(1 / row_sum_alpha[:, np.newaxis])\n",
    "    track_embeddings = normalized_matrix.T @ interaction_matrix\n",
    "    return track_embeddings\n",
    "\n",
    "# Prepare the interaction matrix using Dask\n",
    "def prepare_interaction_matrix_dask(train_df, track_to_idx, num_playlists, num_tracks):\n",
    "    rows, cols, data = [], [], []\n",
    "\n",
    "    @delayed\n",
    "    def process_partition(partition):\n",
    "        partition_rows, partition_cols, partition_data = [], [], []\n",
    "        for pid, tracks in partition.groupby('playlist_id')['track_uri']:\n",
    "            for track in tracks:\n",
    "                partition_rows.append(pid)  # Playlist ID\n",
    "                partition_cols.append(track_to_idx[track])  # Convert track URI to index\n",
    "                partition_data.append(1)\n",
    "        return partition_rows, partition_cols, partition_data\n",
    "\n",
    "    partitions = [\n",
    "        process_partition(partition)\n",
    "        for partition in train_df.to_delayed()\n",
    "    ]\n",
    "\n",
    "    results = delayed(partitions).compute()\n",
    "    for r, c, d in results:\n",
    "        rows.extend(r)\n",
    "        cols.extend(c)\n",
    "        data.extend(d)\n",
    "\n",
    "    interaction_matrix = csr_matrix((data, (rows, cols)), shape=(num_playlists, num_tracks))\n",
    "    return interaction_matrix\n",
    "\n",
    "# Evaluate FISM using Dask\n",
    "def evaluate_fism_dask(train_df, test_df, alpha=0.5):\n",
    "    # Convert track URIs to indices\n",
    "    all_tracks = dd.concat([train_df['track_uri'], test_df['track_uri']]).unique().compute()\n",
    "    track_to_idx = {track: idx for idx, track in enumerate(all_tracks)}\n",
    "    idx_to_track = {idx: track for track, idx in track_to_idx.items()}\n",
    "\n",
    "    num_playlists = train_df['playlist_id'].nunique().compute()\n",
    "    num_tracks = len(all_tracks)\n",
    "\n",
    "    # Prepare interaction matrix\n",
    "    print(\"Preparing interaction matrix...\")\n",
    "    interaction_matrix = prepare_interaction_matrix_dask(train_df, track_to_idx, num_playlists, num_tracks)\n",
    "\n",
    "    # Generate FISM embeddings\n",
    "    print(\"Generating FISM embeddings...\")\n",
    "    track_embeddings = generate_fism_embeddings_sparse(interaction_matrix, alpha)\n",
    "\n",
    "    # Select a single test playlist\n",
    "    test_playlists = test_df.groupby('playlist_id')['track_uri'].apply(list).compute()\n",
    "    playlist_id = list(test_playlists.keys())[0]\n",
    "    tracks = test_playlists[playlist_id]\n",
    "\n",
    "    # Split tracks into seed and holdout\n",
    "    seed_tracks = tracks[:max(1, len(tracks) // 2)]\n",
    "    holdout_tracks = set(tracks) - set(seed_tracks)\n",
    "\n",
    "    # Find seed indices\n",
    "    seed_indices = [track_to_idx[track] for track in seed_tracks if track in track_to_idx]\n",
    "\n",
    "    # Recommend tracks\n",
    "    playlist_idx = playlist_id  # Assume playlist_id matches interaction matrix row\n",
    "    scores = track_embeddings[:, playlist_idx].toarray().flatten()\n",
    "    recommended_indices = np.argsort(-scores)\n",
    "    recommended_tracks = [idx_to_track[idx] for idx in recommended_indices if idx in idx_to_track]\n",
    "\n",
    "    # Calculate metrics\n",
    "    r_precision = len(set(recommended_tracks) & holdout_tracks) / len(holdout_tracks) if holdout_tracks else 0\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, track in enumerate(recommended_tracks) if track in holdout_tracks)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(len(holdout_tracks)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"FISM Model - Playlist ID: {playlist_id}\")\n",
    "    print(f\"R-Precision: {r_precision}\")\n",
    "    print(f\"NDCG: {ndcg}\")\n",
    "\n",
    "# Load Dask DataFrames\n",
    "train_df = dd.from_pandas(train_df, npartitions=4)\n",
    "test_df = dd.from_pandas(test_df, npartitions=2)\n",
    "\n",
    "# Example Usage\n",
    "evaluate_fism_dask(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Results\n",
    "results = {\n",
    "    \"Model\": [\"Jaccard\", \"Item2Vec\", \"FISM\"],\n",
    "    \"R-Precision\": [r_precision_jaccard, r_precision_item2vec, r_precision_fism],\n",
    "    \"NDCG\": [ndcg_jaccard, ndcg_item2vec, ndcg_fism]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
